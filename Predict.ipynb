{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Predict.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOstHmlXBqjOFPSkPtRN0+j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2AhHhYr5WClz","colab_type":"code","colab":{}},"source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsFVTLxtWNxV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"268e2069-1335-40c8-976e-633edabd23e7","executionInfo":{"status":"ok","timestamp":1589956033303,"user_tz":-600,"elapsed":5104,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}}},"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"There are {} GPUs available.\".format(torch.cuda.device_count()))\n","    print(\"We will use GPU {}\".format(torch.cuda.get_device_name(0)))\n","else:\n","    print(\"There is no GPU available, using the CPU instead!\")\n","    device = torch.device(\"cpu\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["There are 1 GPUs available.\n","We will use GPU Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BDVtJZqfWUb1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"4c6720c5-5220-4eed-de1f-03cbdcd36ae1","executionInfo":{"status":"ok","timestamp":1589956057166,"user_tz":-600,"elapsed":21452,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}}},"source":["!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting gputil\n","  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=9f676597269e6aefd9dca37c3300dd1c7624d4b2148fda22e8341ce0a2d05415\n","  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Gen RAM Free: 12.7 GB  | Proc size: 281.2 MB\n","GPU RAM Free: 11430MB | Used: 11MB | Util   0% | Total 11441MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S3jIQfVe7s6s","colab_type":"code","outputId":"42c07cf4-e4b7-46fe-b2a3-b79a6acb5cd9","executionInfo":{"status":"ok","timestamp":1589957238329,"user_tz":-600,"elapsed":4320,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#Testing the change\n","import pandas as pd\n","train_with_1 = pd.read_csv('modified_train.csv')\n","train_with_1.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>date</th>\n","      <th>user</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Fri Jun 05 22:04:23 PDT 2009</td>\n","      <td>JGoldsborough</td>\n","      <td>@jbtaylor WIth ya. &amp;quot;I'd like a Palm Pre, ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Sat Jun 06 03:12:21 PDT 2009</td>\n","      <td>Psioui</td>\n","      <td>felt the earthquake this afternoon, it seems t...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Sat May 30 19:02:49 PDT 2009</td>\n","      <td>adriville</td>\n","      <td>Ruffles on shirts are like so in, me Likey</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Thu Jun 25 05:59:18 PDT 2009</td>\n","      <td>Blondie128</td>\n","      <td>Pretty bad night into a crappy morning....FML!...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Sat May 30 11:16:35 PDT 2009</td>\n","      <td>khrabrov</td>\n","      <td>@dcbriccetti yeah, what a clear view!</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  ... target\n","0   0  ...    1.0\n","1   1  ...    1.0\n","2   2  ...    1.0\n","3   3  ...    0.0\n","4   4  ...    1.0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"U3GHiVGz7s9U","colab_type":"code","colab":{}},"source":["#Defining the generate bigrams method for the Fast_Text class\n","def generate_bigrams(x):\n","    n_grams = set(zip(*[x[i:] for i in range(2)]))\n","    for n_gram in n_grams:\n","        x.append(' '.join(n_gram))\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IX3pFAYI7tAF","colab_type":"code","colab":{}},"source":["#Getting the relevant imports and the fields for reading training data\n","import torch\n","from torchtext import data\n","from torchtext import datasets\n","import random\n","import pandas as pd\n","import numpy as np\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(preprocessing = generate_bigrams)\n","TARGET = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SO1I9jt7tEg","colab_type":"code","colab":{}},"source":["#Defining the fields for reading train.csv\n","fields_train = [(None, None), (None, None), (None, None), ('text', TEXT),('target', TARGET)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"14_B5KDM7tIv","colab_type":"code","colab":{}},"source":["#Reading train.csv\n","train_data = data.TabularDataset(path = 'modified_train.csv',\n","                                 format = 'csv',\n","                                 fields = fields_train,\n","                                 skip_header = True\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG6PvJYo7tLd","colab_type":"code","colab":{}},"source":["#Creating validation set\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCnlT8Pe7tOO","colab_type":"code","outputId":"e38004d9-c09f-4e45-ad2d-0fcfab2361a1","executionInfo":{"status":"ok","timestamp":1589957753370,"user_tz":-600,"elapsed":471408,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Getting the pre-trained word embeddings and building the vocab\n","MAX_VOCAB_SIZE = 25000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_)\n","\n","TARGET.build_vocab(train_data)"],"execution_count":9,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n","100%|█████████▉| 399951/400000 [00:21<00:00, 17045.34it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_5g1P4f01qU1","colab_type":"code","colab":{}},"source":["#defining the Fast_Text Class\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FastText(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        \n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","        \n","    def forward(self, text):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.embedding(text)\n","                \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        embedded = embedded.permute(1, 0, 2)\n","        \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n","        \n","        #pooled = [batch size, embedding_dim]\n","                \n","        return self.fc(pooled) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ02TlSy3KDR","colab_type":"code","colab":{}},"source":["# defining our models and the relevant parameters\n","model = FastText(25002, 100, 1, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Em0lx479ySrg","colab_type":"code","outputId":"3b94485f-add8-4687-a312-c04e3b0c1273","executionInfo":{"status":"ok","timestamp":1589957836562,"user_tz":-600,"elapsed":9313,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import torch\n","\n","model.load_state_dict(torch.load(\"tut3-model.pt\"))\n","model.to(device)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FastText(\n","  (embedding): Embedding(25002, 100, padding_idx=1)\n","  (fc): Linear(in_features=100, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"MY1dkqznyiUO","colab_type":"code","colab":{}},"source":["#Inference method\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = generate_bigrams(sentence.split())\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXRtf_l5-DQj","colab_type":"code","colab":{}},"source":["#defining the accuracy calculation method\n","def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1S5F1sT0gDt","colab_type":"code","outputId":"b6ea12e7-8457-4209-de15-1290c7a136fa","executionInfo":{"status":"ok","timestamp":1589957847310,"user_tz":-600,"elapsed":1595,"user":{"displayName":"troy cao","photoUrl":"","userId":"07726115307262362470"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Running inference on test\n","preds = []\n","\n","test_data = pd.read_csv('tweet_test.csv')\n","\n","# print(test_data[\"text\"][0])\n","for i in range(len(test_data)):\n","    preds.append((int(predict_sentiment(model, str(test_data[\"text\"][i]))>0.5)))\n","preds = torch.FloatTensor(preds)\n","labels = torch.tensor(test_data[\"label\"])\n","# ids = test_data['Id']\n","# dict = {'Id': ids, 'target': preds}\n","# df = pd.DataFrame(dict) \n","acc = binary_accuracy(preds, labels)\n","\n","print(f'\\tThe Test. Acc is: {acc*100:.2f}%')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\tThe Test. Acc is: 81.20%\n"],"name":"stdout"}]}]}