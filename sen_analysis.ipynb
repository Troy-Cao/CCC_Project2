{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "sen_analysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwDSLbhXMise",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"There are {} GPUs available.\".format(torch.cuda.device_count()))\n",
        "    print(\"We will use GPU {}\".format(torch.cuda.get_device_name(0)))\n",
        "else:\n",
        "    print(\"There is no GPU available, using the CPU instead!\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpMZRsDRKaPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_content = pd.read_csv(\"train.csv\")\n",
        "test_content = pd.read_csv(\"test.csv\")\n",
        "\n",
        "def read_file(contents):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(contents)):\n",
        "      record = contents.iloc[i, :]\n",
        "      text_list.append(record[\"text\"])\n",
        "      label_list.append(record[\"label\"])\n",
        "    return text_list, label_list\n",
        "\n",
        "train_text, train_label = read_file(train_content)\n",
        "test_text, test_label = read_file(test_content)\n",
        "print(len(train_text))\n",
        "print(len(test_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH0nzRWtKaPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we gonna load the module\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oZZ70-FKaPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQhGAfdFKaPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "nltk.download('punkt')\n",
        "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nbZXaOEKaPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_para_to_id(contents, para_length):\n",
        "    contents_ids_list = []\n",
        "    attentions_list = []\n",
        "    for content in contents:\n",
        "        word_count = 0\n",
        "        content_list = []\n",
        "        attention_list = []\n",
        "        sentences = sent_segmenter.tokenize(content)\n",
        "        for sentence in sentences:\n",
        "            while word_count < para_length:\n",
        "                encoded_con = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "                content_list.extend(encoded_con)\n",
        "                word_count += len(encoded_con)\n",
        "        if len(content_list) > para_length:\n",
        "            content_list = content_list[:para_length]\n",
        "            attention_list.extend([1] * para_length)\n",
        "        else:\n",
        "            content_list.extend([0] * (para_length - len(content_list)))\n",
        "            attention_list.extend([1] * len(content_list))\n",
        "            attention_list.extend([0] * (para_length - len(content_list)))\n",
        "        contents_ids_list.append(content_list)\n",
        "        attentions_list.append(attention_list)\n",
        "    return contents_ids_list, attentions_list\n",
        "\n",
        "\n",
        "train_ids_list, train_attention_list = convert_para_to_id(train_text, 200)\n",
        "print(len(train_ids_list))\n",
        "print(len(train_attention_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai2_s70MKaPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Covert label list to Tensor\n",
        "import torch\n",
        "\n",
        "labels = torch.LongTensor(train_label)\n",
        "input_ids = torch.LongTensor(train_ids_list)\n",
        "attention_mask = torch.LongTensor(train_attention_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuwJJ0_fKaP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the traiing inputs into a TensroDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "train_data = TensorDataset(input_ids, attention_mask, labels)\n",
        "\n",
        "\n",
        "train_data_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=100)\n",
        "print(len(train_data_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7wqO6xDKaP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Model\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels = 2,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = False\n",
        ")\n",
        "\n",
        "print(\"The Model Loading Completed!...\")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI1Gl3GSKaP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps=1e-8)\n",
        "print(\"Optimizer Loading Completed!...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyW4AAvFKaP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZxXYLALKaQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a helper function for calculating Accuracy\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zWSZXiPKaQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the dev dataset\n",
        "dev_text = test_text[:100]\n",
        "dev_label = test_label[:100]\n",
        "\n",
        "\n",
        "dev_ids_list, dev_attention_list = convert_para_to_id(dev_text, 200)\n",
        "print(len(dev_ids_list))\n",
        "\n",
        "dev_labels = torch.LongTensor(dev_label)\n",
        "dev_input_ids = torch.LongTensor(dev_ids_list)\n",
        "dev_attention_mask = torch.LongTensor(dev_attention_list)\n",
        "dev_data = TensorDataset(dev_input_ids, dev_attention_mask , dev_labels)\n",
        "\n",
        "\n",
        "dev_data_loader = DataLoader(dev_data, sampler=RandomSampler(dev_data), batch_size=10)\n",
        "print(len(dev_data_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AAs3_8nKaQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now training\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "\n",
        "training_stats = [] # used to store the training information\n",
        "\n",
        "\n",
        "for epoch_i in range(epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
        "    print(\"Training...\")\n",
        "    \n",
        "    total_train_loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for step, batch in enumerate(train_data_loader):\n",
        "        \n",
        "        if (step + 1) % 10 == 0 and not step == 0:\n",
        "            print(\"Batch {} of {}\".format(step, len(train_data_loader)))\n",
        "        \n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_input_mask = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        loss, logits = model(batch_input_ids, token_type_ids=None,\n",
        "                            attention_mask=batch_input_mask,\n",
        "                            labels=batch_labels)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "    \n",
        "    avg_train_loss = total_train_loss / len(train_data_loader)\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\" Average Training Loss is {:2f}\".format(avg_train_loss))\n",
        "    \n",
        "    # Now perform validation\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    \n",
        "    for batch in dev_data_loader:\n",
        "        dev_b_input_ids = batch[0].to(device)\n",
        "        dev_b_input_mask = batch[1].to(device)\n",
        "        dev_b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(dev_b_input_ids, \n",
        "                                 token_type_ids=None, \n",
        "                                attention_mask=dev_b_input_mask,\n",
        "                                labels=dev_b_labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = dev_b_labels.to(\"cpu\").numpy()\n",
        "        \n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    avg_val_loss = total_eval_loss / len(dev_data_loader)\n",
        "    avg_val_accuracy = total_eval_accuracy / len(dev_data_loader)\n",
        "    print(\"Validation loss :{}\".format(avg_val_loss))\n",
        "    print(\"Accuracy is {}\".format(avg_val_accuracy))\n",
        "    \n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "        }\n",
        "    )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training Complete!...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPRFYgjpKaQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, \"BERT_v1.0.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}